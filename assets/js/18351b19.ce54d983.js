"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3663],{2759:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"appendices/glossary","title":"Glossary of Terms","description":"This appendix provides definitions for key terms used throughout the Physical AI & Humanoid Robotics curriculum.","source":"@site/docs/appendices/01-glossary.md","sourceDirName":"appendices","slug":"/appendices/glossary","permalink":"/Physical-AI---Humanoid-Robotics/docs/appendices/glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/abdullahnisar05/Physical-AI---Humanoid-Robotics/edit/main/website/docs/appendices/01-glossary.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Self-Assessment Quizzes","permalink":"/Physical-AI---Humanoid-Robotics/docs/assessments/quizzes"},"next":{"title":"References and Resources","permalink":"/Physical-AI---Humanoid-Robotics/docs/appendices/references"}}');var o=i(4848),s=i(8453);const r={sidebar_position:1},a="Glossary of Terms",l={},c=[{value:"A",id:"a",level:2},{value:"B",id:"b",level:2},{value:"C",id:"c",level:2},{value:"D",id:"d",level:2},{value:"E",id:"e",level:2},{value:"F",id:"f",level:2},{value:"G",id:"g",level:2},{value:"H",id:"h",level:2},{value:"I",id:"i",level:2},{value:"J",id:"j",level:2},{value:"K",id:"k",level:2},{value:"L",id:"l",level:2},{value:"M",id:"m",level:2},{value:"N",id:"n",level:2},{value:"O",id:"o",level:2},{value:"P",id:"p",level:2},{value:"R",id:"r",level:2},{value:"S",id:"s",level:2},{value:"T",id:"t",level:2},{value:"U",id:"u",level:2},{value:"V",id:"v",level:2},{value:"W",id:"w",level:2},{value:"X",id:"x",level:2},{value:"Y",id:"y",level:2},{value:"Z",id:"z",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"glossary-of-terms",children:"Glossary of Terms"})}),"\n",(0,o.jsx)(n.p,{children:"This appendix provides definitions for key terms used throughout the Physical AI & Humanoid Robotics curriculum."}),"\n",(0,o.jsx)(n.h2,{id:"a",children:"A"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Action (in ROS 2)"}),": A communication pattern in ROS 2 for long-running tasks that provides feedback during execution and returns a result upon completion. Actions are ideal for tasks like navigation or manipulation that take time to complete."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Artificial Intelligence (AI)"}),": The simulation of human intelligence processes by machines, especially computer systems. In robotics, AI enables robots to perceive, reason, learn, and adapt to their environment."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Autonomous"}),": Capable of acting independently without human intervention. An autonomous robot can perceive its environment and make decisions without continuous human guidance."]}),"\n",(0,o.jsx)(n.h2,{id:"b",children:"B"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Balance Control"}),": The ability of a humanoid robot to maintain its center of mass within its support polygon to prevent falling. Critical for bipedal locomotion."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Base of Support"}),": The area beneath an object or person that includes every point of contact with the supporting surface. For humanoid robots, this is the area between the feet."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Behavior Tree"}),": A hierarchical model used in robotics and AI to structure the execution of tasks. It provides a way to organize complex behaviors in a modular and reusable manner."]}),"\n",(0,o.jsx)(n.h2,{id:"c",children:"C"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Center of Mass (CoM)"}),": The point in a robot where all of its mass can be considered to be concentrated. Critical for balance and stability calculations in humanoid robots."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Computer Vision"}),": A field of artificial intelligence that trains computers to interpret and understand the visual world. In robotics, it enables robots to identify objects, people, and navigate environments."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Control Theory"}),": The interdisciplinary branch of engineering and mathematics that deals with the behavior of dynamical systems with inputs, and how their behavior is modified by feedback."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Convolutional Neural Network (CNN)"}),": A class of deep neural networks commonly applied to analyzing visual imagery. Widely used in robotics for object detection and scene understanding."]}),"\n",(0,o.jsx)(n.h2,{id:"d",children:"D"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Deep Learning"}),": A subset of machine learning based on artificial neural networks with representation learning. Used extensively in robotics for perception and control tasks."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Degree of Freedom (DOF)"}),": The number of independent movements a mechanical system can make. For humanoid robots, each joint typically contributes one or more degrees of freedom."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Digital Twin"}),": A virtual replica of a physical system that mirrors its characteristics, behaviors, and responses in real-time. Used for safe, cost-effective development and testing."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Domain Randomization"}),": A technique in simulation where environment parameters (textures, lighting, colors) are randomized to improve the transfer of models from simulation to reality."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Dynamic Movement Primitive (DMP)"}),": A method for learning and reproducing robot movements. Used in humanoid robotics for generating smooth, adaptive motions."]}),"\n",(0,o.jsx)(n.h2,{id:"e",children:"E"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Embodied AI"}),": Artificial intelligence that is integrated with a physical body that interacts with the real world. The physical embodiment influences and is influenced by the AI's behavior."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"End Effector"}),": The device at the end of a robot arm that interacts with the environment. For humanoid robots, this is typically a hand designed for grasping and manipulation."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Episodic Memory"}),": Memory of autobiographical episodes or specific events that occurred at particular times and places. Relevant in long-term human-robot interaction."]}),"\n",(0,o.jsx)(n.h2,{id:"f",children:"F"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Forward Kinematics"}),": The process of determining the position and orientation of the end effector based on the joint angles of a robot. The opposite of inverse kinematics."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Frame of Reference"}),": A coordinate system used to represent and measure the position and orientation of objects in space. Essential for robot navigation and manipulation."]}),"\n",(0,o.jsx)(n.h2,{id:"g",children:"G"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Gazebo"}),": A 3D dynamic simulator with robust physics engine, high-quality graphics, and sensor simulation capabilities. Widely used in robotics for testing and development."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"General Artificial Intelligence (AGI)"}),": Hypothetical AI that matches or exceeds human intelligence across the full range of cognitive tasks."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Generative Adversarial Network (GAN)"}),": A class of machine learning frameworks where two neural networks contest with each other. Used in robotics for synthetic data generation."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Geometry"}),": In ROS, refers to the geometric shapes and transformations used in robot modeling, typically handled by the TF (Transform) library."]}),"\n",(0,o.jsx)(n.h2,{id:"h",children:"H"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Hardware Acceleration"}),": The use of computer hardware specially made to execute specific tasks, used to speed up computations in robotics perception and control."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Human-Robot Interaction (HRI)"}),": The study of interactions between humans and robots, focusing on design, development, and evaluation of robots for human use."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Humanoid Robot"}),": A robot with a body structure that mimics the human form, typically featuring a head, torso, two arms, and two legs."]}),"\n",(0,o.jsx)(n.h2,{id:"i",children:"I"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Inverse Kinematics"}),": The process of determining the joint angles required to place the end effector at a desired position and orientation. The reverse of forward kinematics."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Isaac Sim"}),": NVIDIA's high-fidelity simulation environment built on the Omniverse platform, designed for robotics development and testing."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS"}),": A collection of hardware-accelerated packages that bridge the gap between NVIDIA's GPU computing platform and ROS 2."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Iteration"}),": A repetition of a process in robotics programming, often used in learning algorithms and control systems."]}),"\n",(0,o.jsx)(n.h2,{id:"j",children:"J"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Joint"}),": A connection between two or more links in a robot that allows relative motion. The degrees of freedom of a robot are determined by its joints."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Joint Space"}),": The space defined by the joint angles of a robot. Often contrasted with Cartesian space."]}),"\n",(0,o.jsx)(n.h2,{id:"k",children:"K"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Kinematics"}),": The branch of mechanics concerned with the motion of objects without reference to the forces that cause the motion. In robotics, it deals with the geometric aspects of motion."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Kinesthetic Teaching"}),": A method of teaching robot motions by physically guiding the robot through the desired motion path."]}),"\n",(0,o.jsx)(n.h2,{id:"l",children:"L"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Large Language Model (LLM)"}),": Advanced AI models trained on vast amounts of text data, capable of understanding and generating human-like text. Used in robotics for natural language interaction."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Learning Rate"}),": A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"LiDAR"}),": Light Detection and Ranging. A remote sensing method that uses light in the form of a pulsed laser to measure distances. Commonly used in robotics for navigation and mapping."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Localization"}),": The process of determining the robot's position and orientation within a known or unknown environment."]}),"\n",(0,o.jsx)(n.h2,{id:"m",children:"M"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Machine Learning (ML)"}),": A subset of AI that provides systems the ability to automatically learn and improve from experience without being explicitly programmed."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": The ability of a robot to physically interact with objects in its environment, typically through grasping, lifting, or moving objects."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Middleware"}),": Software that provides common services and capabilities to applications beyond what's offered by the operating system. ROS 2 serves as middleware for robotics applications."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Model Predictive Control (MPC)"}),": An advanced control method that uses a model of the system to predict future behavior and optimize control actions."]}),"\n",(0,o.jsx)(n.h2,{id:"n",children:"N"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Navigation"}),": The ability of a robot to move from one location to another in an environment, often involving path planning and obstacle avoidance."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Neural Network"}),": A computing system inspired by the biological neural networks that constitute animal brains. Used extensively in robotics for perception and control."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Non-Holonomic Constraint"}),": A constraint on a robot's motion that cannot be integrated to give a constraint only on position. Common in wheeled robots."]}),"\n",(0,o.jsx)(n.h2,{id:"o",children:"O"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Odometry"}),": The use of data from motion sensors to estimate change in robot position over time. Fundamental for robot navigation."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Omniverse"}),": NVIDIA's platform for real-time collaboration and simulation, serving as the foundation for Isaac Sim."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Operational Space"}),": The space in which the robot's end effector operates, typically Cartesian space with position and orientation."]}),"\n",(0,o.jsx)(n.h2,{id:"p",children:"P"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": The ability of a robot to interpret and understand its environment through sensors and processing algorithms."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Planning"}),": The process of determining a sequence of actions to achieve a goal. Includes path planning, motion planning, and task planning."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Point Cloud"}),": A set of data points in space, representing the external surface of an object or environment. Commonly generated by LiDAR and stereo vision."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Prompt Engineering"}),": The practice of crafting inputs to guide large language models to produce desired outputs, particularly important in robotics applications."]}),"\n",(0,o.jsx)(n.h2,{id:"r",children:"R"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Real-time"}),": Systems that must respond to inputs within strict time constraints. Critical for robot control and safety systems."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reinforcement Learning"}),": A type of machine learning where an agent learns to make decisions by performing actions and receiving rewards or penalties."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Robot Operating System (ROS)"}),": Flexible framework for writing robot software. It provides services designed for a heterogeneous computer cluster."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Robotics Middleware"}),": Software layer that facilitates communication between different components of a robotic system, with ROS 2 being the current standard."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"ROS 2"}),": The second generation of the Robot Operating System, featuring improved security, real-time capabilities, and support for commercial development."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Runtime"}),": The period during which a program is executing, as opposed to compile time. Important for understanding robot system performance."]}),"\n",(0,o.jsx)(n.h2,{id:"s",children:"S"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sensor Fusion"}),": The process of combining data from multiple sensors to achieve better accuracy and reliability than could be achieved by using a single sensor."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sim-to-Real Transfer"}),": The process of transferring knowledge, models, or behaviors learned in simulation to real-world robotic systems."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Simulation"}),": The imitation of the operation of a real-world process or system over time, essential for robotics development and testing."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),": The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"State Estimation"}),": The process of determining the internal state of a system from measurements, critical for robot control and navigation."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Stereo Vision"}),": A technique that extracts 3D information from digital images taken by two or more cameras, used for depth perception in robotics."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"System Integration"}),": The process of bringing together the component subsystems into one system and ensuring that the subsystems function together as a unit."]}),"\n",(0,o.jsx)(n.h2,{id:"t",children:"T"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Task Planning"}),": The process of decomposing high-level goals into sequences of primitive actions that a robot can execute."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"TF (Transform Library)"}),": A package in ROS that keeps track of multiple coordinate frames over time, essential for robot perception and navigation."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Trajectory"}),": A path that includes timing information, describing not just where a robot should go but when it should be at each point."]}),"\n",(0,o.jsx)(n.h2,{id:"u",children:"U"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Unified Robot Description Format (URDF)"}),": An XML format for representing a robot model, including its physical and visual properties, joints, and other components."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Universal Scene Description (USD)"}),": NVIDIA's format for 3D computer graphics data interchange, used extensively in Isaac Sim."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Uncanny Valley"}),": The hypothesis that human replicas that appear almost, but not exactly, like real human beings cause a feeling of eeriness and repulsion among human observers."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Upper Body"}),": The portion of a humanoid robot consisting of the torso, arms, and head, responsible for manipulation and interaction tasks."]}),"\n",(0,o.jsx)(n.h2,{id:"v",children:"V"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Variable Impedance Control"}),": A control method that allows the robot to vary its mechanical impedance (resistance to motion) to adapt to different tasks and environments."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"}),": A paradigm combining computer vision, natural language processing, and robotic action planning in a unified framework."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Virtual Reality (VR)"}),": A simulated experience that can be similar to or completely different from the real world, sometimes used in robotics development."]}),"\n",(0,o.jsx)(n.h2,{id:"w",children:"W"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Waypoint"}),": A reference or marker in physical space used for navigation, typically represented as coordinates in a map."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Whole-Body Control"}),": A control approach that considers the entire robot as a single system, optimizing for all tasks and constraints simultaneously."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Workspace"}),": The volume in space that a robot can reach with its end effector."]}),"\n",(0,o.jsx)(n.h2,{id:"x",children:"X"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Xacro"}),": An XML macro language for generating URDF files, allowing for more flexible and reusable robot descriptions."]}),"\n",(0,o.jsx)(n.h2,{id:"y",children:"Y"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Yaw"}),": The rotation of a robot around its vertical axis, one of the three rotational degrees of freedom (roll, pitch, yaw)."]}),"\n",(0,o.jsx)(n.h2,{id:"z",children:"Z"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Zero Moment Point (ZMP)"}),": A concept used in robotics and biomechanics to maintain balance in walking robots by ensuring the net moment of the ground reaction forces is zero."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Zone of proximal development"}),": A concept from educational psychology relevant to human-robot interaction, referring to the difference between what a learner can do without help and what they can achieve with guidance."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);